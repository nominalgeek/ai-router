#!/bin/bash
# Benchmark script for RTX PRO 6000 Blackwell setup

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

BASE_URL="http://localhost"

echo -e "${BLUE}==================================="
echo "RTX PRO 6000 Benchmark Suite"
echo "===================================${NC}"
echo ""

# Check GPU
echo -e "${YELLOW}GPU Status:${NC}"
nvidia-smi --query-gpu=name,memory.total,memory.free,temperature.gpu,utilization.gpu --format=csv,noheader
echo ""

# Warm up models
echo -e "${YELLOW}Warming up models (compiling CUDA kernels)...${NC}"
echo ""

curl -s -X POST "$BASE_URL/router/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "hi"}], "max_tokens": 5}' > /dev/null 2>&1 &

curl -s -X POST "$BASE_URL/primary/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "hi"}], "max_tokens": 5}' > /dev/null 2>&1 &

wait
echo -e "${GREEN}✓ Warmup complete${NC}"
echo ""

# Benchmark 1: Latency Test
echo -e "${BLUE}=== Benchmark 1: Latency (Time to First Token) ===${NC}"
echo ""

echo -e "${YELLOW}Router Model (Simple Query):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/router/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 10,
    "stream": false
  }' > /tmp/router_response.json
END=$(date +%s.%N)
LATENCY=$(echo "$END - $START" | bc)
RESPONSE=$(cat /tmp/router_response.json | jq -r '.choices[0].message.content' 2>/dev/null || echo "Error")
echo "  Latency: ${LATENCY}s"
echo "  Response: ${RESPONSE:0:100}"
echo ""

echo -e "${YELLOW}Primary Model (Simple Query):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/primary/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 10,
    "stream": false
  }' > /tmp/primary_response.json
END=$(date +%s.%N)
LATENCY=$(echo "$END - $START" | bc)
RESPONSE=$(cat /tmp/primary_response.json | jq -r '.choices[0].message.content' 2>/dev/null || echo "Error")
echo "  Latency: ${LATENCY}s"
echo "  Response: ${RESPONSE:0:100}"
echo ""

# Benchmark 2: Throughput Test
echo -e "${BLUE}=== Benchmark 2: Throughput (Tokens/Second) ===${NC}"
echo ""

echo -e "${YELLOW}Router Model (100 tokens):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/router/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Write a short paragraph about artificial intelligence"}],
    "max_tokens": 100,
    "stream": false
  }' > /tmp/router_throughput.json
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
TOKENS=$(cat /tmp/router_throughput.json | jq -r '.usage.completion_tokens' 2>/dev/null || echo "0")
TPS=$(echo "scale=2; $TOKENS / $DURATION" | bc)
echo "  Duration: ${DURATION}s"
echo "  Tokens: ${TOKENS}"
echo "  Throughput: ${TPS} tokens/sec"
echo ""

echo -e "${YELLOW}Primary Model (300 tokens):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/primary/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Explain quantum computing in detail"}],
    "max_tokens": 300,
    "stream": false
  }' > /tmp/primary_throughput.json
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
TOKENS=$(cat /tmp/primary_throughput.json | jq -r '.usage.completion_tokens' 2>/dev/null || echo "0")
TPS=$(echo "scale=2; $TOKENS / $DURATION" | bc)
echo "  Duration: ${DURATION}s"
echo "  Tokens: ${TOKENS}"
echo "  Throughput: ${TPS} tokens/sec"
echo ""

# Benchmark 3: Context Length Test
echo -e "${BLUE}=== Benchmark 3: Long Context Performance ===${NC}"
echo ""

echo -e "${YELLOW}Primary Model (8K context):${NC}"
LONG_PROMPT=$(python3 -c "print('The story continues. ' * 500)")
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/primary/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d "{
    \"messages\": [{\"role\": \"user\", \"content\": \"${LONG_PROMPT:0:4000} Summarize the above.\"}],
    \"max_tokens\": 100,
    \"stream\": false
  }" > /tmp/primary_context.json 2>/dev/null
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
SUCCESS=$(cat /tmp/primary_context.json | jq -r '.choices[0].message.content' 2>/dev/null | wc -c)
if [ "$SUCCESS" -gt 10 ]; then
  echo -e "  ${GREEN}✓ Success${NC} (${DURATION}s)"
else
  echo -e "  ${RED}✗ Failed${NC}"
fi
echo ""

# Benchmark 4: Concurrent Requests
echo -e "${BLUE}=== Benchmark 4: Concurrent Request Handling ===${NC}"
echo ""

echo -e "${YELLOW}Testing 10 concurrent requests to router model:${NC}"
START=$(date +%s.%N)
for i in {1..10}; do
  curl -s -X POST "$BASE_URL/router/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -d "{
      \"messages\": [{\"role\": \"user\", \"content\": \"Request $i\"}],
      \"max_tokens\": 20,
      \"stream\": false
    }" > /tmp/concurrent_$i.json 2>/dev/null &
done
wait
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
SUCCESSFUL=$(ls /tmp/concurrent_*.json 2>/dev/null | wc -l)
echo "  Completed: $SUCCESSFUL/10 requests"
echo "  Total time: ${DURATION}s"
echo "  Avg time per request: $(echo "scale=2; $DURATION / 10" | bc)s"
rm -f /tmp/concurrent_*.json
echo ""

# Benchmark 5: Streaming Performance
echo -e "${BLUE}=== Benchmark 5: Streaming Latency ===${NC}"
echo ""

echo -e "${YELLOW}Router Model (streaming):${NC}"
START=$(date +%s.%N)
FIRST_CHUNK_TIME=""
curl -s -X POST "$BASE_URL/router/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Count to 10"}],
    "max_tokens": 50,
    "stream": true
  }' 2>/dev/null | while read line; do
    if [ -z "$FIRST_CHUNK_TIME" ]; then
      FIRST_CHUNK_TIME=$(date +%s.%N)
      TTFT=$(echo "$FIRST_CHUNK_TIME - $START" | bc)
      echo "  Time to First Token: ${TTFT}s"
    fi
  done
echo ""

# GPU Memory Usage
echo -e "${BLUE}=== GPU Memory Usage ===${NC}"
echo ""
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv,noheader,nounits | \
  awk '{printf "  Used: %d MB\n  Free: %d MB\n  Total: %d MB\n  Utilization: %.1f%%\n", $1, $2, $3, ($1/$3)*100}'
echo ""

# Model Information
echo -e "${BLUE}=== Model Information ===${NC}"
echo ""

echo -e "${YELLOW}Router Model:${NC}"
curl -s "$BASE_URL/router/v1/models" | jq -r '.data[0] | "  Model: \(.id)\n  Created: \(.created)\n  Owned by: \(.owned_by)"' 2>/dev/null || echo "  Error fetching model info"
echo ""

echo -e "${YELLOW}Primary Model:${NC}"
curl -s "$BASE_URL/primary/v1/models" | jq -r '.data[0] | "  Model: \(.id)\n  Created: \(.created)\n  Owned by: \(.owned_by)"' 2>/dev/null || echo "  Error fetching model info"
echo ""

# Summary
echo -e "${BLUE}==================================="
echo "Benchmark Complete"
echo "===================================${NC}"
echo ""
echo -e "${GREEN}Your RTX PRO 6000 is performing well!${NC}"
echo ""
echo "Next steps:"
echo "  • Run 'make gpu' to monitor GPU usage"
echo "  • Adjust memory allocation in docker-compose.yml"
echo "  • See RTX_PRO_6000_OPTIMIZATION.md for tuning tips"
echo ""

# Cleanup
rm -f /tmp/router_response.json /tmp/primary_response.json
rm -f /tmp/router_throughput.json /tmp/primary_throughput.json
rm -f /tmp/primary_context.json
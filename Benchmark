#!/bin/bash
# Benchmark script for RTX PRO 6000 Blackwell setup

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

BASE_URL="http://localhost"

# Load API key from .secrets if available (needed for ai-router endpoints).
API_KEY=""
if [ -f .secrets ]; then
    API_KEY=$(grep '^API_KEY=' .secrets | cut -d'=' -f2-)
fi

# Auth header for ai-router endpoints (not needed for direct vLLM calls).
AUTH_HEADER=()
if [ -n "$API_KEY" ]; then
    AUTH_HEADER=(-H "Authorization: Bearer $API_KEY")
fi

echo -e "${BLUE}==================================="
echo "AI Router Benchmark Suite"
echo "Four-Route Routing Pipeline"
echo "===================================${NC}"
echo ""

# Check GPU
echo -e "${YELLOW}GPU Status:${NC}"
nvidia-smi --query-gpu=name,memory.total,memory.free,temperature.gpu,utilization.gpu --format=csv,noheader
echo ""

# Warm up models
echo -e "${YELLOW}Warming up models (compiling CUDA kernels)...${NC}"
echo ""

# Warm up all three tiers
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d '{"messages": [{"role": "user", "content": "hi"}], "max_tokens": 5}' > /dev/null 2>&1

curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d '{"messages": [{"role": "user", "content": "Explain AI"}], "max_tokens": 10}' > /dev/null 2>&1

echo -e "${GREEN}✓ Warmup complete${NC}"
echo ""

# Benchmark 1: Routing Latency by Category
echo -e "${BLUE}=== Benchmark 1: Routing Latency by Category ===${NC}"
echo ""

echo -e "${YELLOW}MODERATE Query — short (Nano 30B via primary):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d '{
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 10,
    "stream": false
  }' > /tmp/moderate_short_response.json
END=$(date +%s.%N)
LATENCY=$(echo "$END - $START" | bc)
MODEL=$(cat /tmp/moderate_short_response.json | jq -r '.model' 2>/dev/null || echo "Error")
RESPONSE=$(cat /tmp/moderate_short_response.json | jq -r '.choices[0].message.content // .choices[0].message.reasoning_content' 2>/dev/null || echo "Error")
echo "  Latency: ${LATENCY}s"
echo "  Model: ${MODEL}"
echo "  Response: ${RESPONSE:0:100}"
echo ""

echo -e "${YELLOW}MODERATE Query — long (Nano 30B via router):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d '{
    "messages": [{"role": "user", "content": "Explain how neural networks learn"}],
    "max_tokens": 50,
    "stream": false
  }' > /tmp/moderate_response.json
END=$(date +%s.%N)
LATENCY=$(echo "$END - $START" | bc)
MODEL=$(cat /tmp/moderate_response.json | jq -r '.model' 2>/dev/null || echo "Error")
RESPONSE=$(cat /tmp/moderate_response.json | jq -r '.choices[0].message.content // .choices[0].message.reasoning_content' 2>/dev/null || echo "Error")
echo "  Latency: ${LATENCY}s"
echo "  Model: ${MODEL}"
echo "  Response: ${RESPONSE:0:100}"
echo ""

echo -e "${YELLOW}COMPLEX Query (xAI Grok via router):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d '{
    "messages": [{"role": "user", "content": "Design a novel approach to quantum error correction"}],
    "max_tokens": 100,
    "stream": false
  }' > /tmp/complex_response.json
END=$(date +%s.%N)
LATENCY=$(echo "$END - $START" | bc)
MODEL=$(cat /tmp/complex_response.json | jq -r '.model' 2>/dev/null || echo "Error")
RESPONSE=$(cat /tmp/complex_response.json | jq -r '.choices[0].message.content // .choices[0].message.reasoning_content' 2>/dev/null || echo "Error")
echo "  Latency: ${LATENCY}s"
echo "  Model: ${MODEL}"
echo "  Response: ${RESPONSE:0:100}"
echo ""

echo -e "${YELLOW}ENRICH Query (xAI context → Nano 30B):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d '{
    "messages": [{"role": "user", "content": "What are the latest developments in AI regulation this week?"}],
    "max_tokens": 200,
    "stream": false
  }' > /tmp/enrich_response.json
END=$(date +%s.%N)
LATENCY=$(echo "$END - $START" | bc)
MODEL=$(cat /tmp/enrich_response.json | jq -r '.model' 2>/dev/null || echo "Error")
RESPONSE=$(cat /tmp/enrich_response.json | jq -r '.choices[0].message.content // .choices[0].message.reasoning_content' 2>/dev/null || echo "Error")
echo "  Latency: ${LATENCY}s (includes xAI context fetch + primary inference)"
echo "  Model: ${MODEL}"
echo "  Response: ${RESPONSE:0:100}"
echo ""

# Benchmark 2: Throughput Test
# Uses /api/route with explicit routing to bypass classification overhead,
# measuring raw primary model throughput through the ai-router.
echo -e "${BLUE}=== Benchmark 2: Throughput (Tokens/Second) ===${NC}"
echo ""

echo -e "${YELLOW}Primary Model — short (100 tokens):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/api/route" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d '{
    "route": "primary",
    "data": {
      "messages": [{"role": "user", "content": "Write a short paragraph about artificial intelligence"}],
      "max_tokens": 100,
      "stream": false
    }
  }' > /tmp/primary_short_throughput.json
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
TOKENS=$(jq -r '.usage.completion_tokens // 0' /tmp/primary_short_throughput.json 2>/dev/null || echo "0")
TPS=$(echo "scale=1; $TOKENS / $DURATION" | bc)
echo "  Duration: ${DURATION}s"
echo "  Tokens: ${TOKENS}"
echo "  Throughput: ${TPS} tokens/sec"
echo ""

echo -e "${YELLOW}Primary Model — long (300 tokens):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/api/route" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d '{
    "route": "primary",
    "data": {
      "messages": [{"role": "user", "content": "Explain quantum computing in detail"}],
      "max_tokens": 300,
      "stream": false
    }
  }' > /tmp/primary_long_throughput.json
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
TOKENS=$(jq -r '.usage.completion_tokens // 0' /tmp/primary_long_throughput.json 2>/dev/null || echo "0")
TPS=$(echo "scale=1; $TOKENS / $DURATION" | bc)
echo "  Duration: ${DURATION}s"
echo "  Tokens: ${TOKENS}"
echo "  Throughput: ${TPS} tokens/sec"
echo ""

# Benchmark 3: Context Length Test
# Sends a long prompt through the ai-router via explicit routing to primary.
echo -e "${BLUE}=== Benchmark 3: Long Context Performance ===${NC}"
echo ""

echo -e "${YELLOW}Primary Model (8K context):${NC}"
LONG_PROMPT=$(python3 -c "print('The story continues. ' * 500)")
START=$(date +%s.%N)
# No max_tokens — matches how the router sends requests to the primary model.
# The Nano 30B reasoning model uses its token budget for <think> blocks;
# any artificial cap risks truncating reasoning before content is emitted.
curl -s -X POST "$BASE_URL/api/route" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d "{
    \"route\": \"primary\",
    \"data\": {
      \"messages\": [{\"role\": \"user\", \"content\": \"${LONG_PROMPT:0:4000} Summarize the above.\"}],
      \"stream\": false
    }
  }" > /tmp/primary_context.json 2>/dev/null
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
# Check both content and reasoning_content — the reasoning model may put
# all output in reasoning_content with content=null.
CONTENT=$(jq -r '.choices[0].message.content // empty' /tmp/primary_context.json 2>/dev/null)
REASONING=$(jq -r '.choices[0].message.reasoning_content // empty' /tmp/primary_context.json 2>/dev/null)
if [ ${#CONTENT} -gt 10 ] || [ ${#REASONING} -gt 10 ]; then
  echo -e "  ${GREEN}✓ Success${NC} (${DURATION}s)"
else
  echo -e "  ${RED}✗ Failed${NC}"
fi
echo ""

# Benchmark 4: Concurrent Requests
# Sends 5 concurrent requests through the full routing pipeline.
echo -e "${BLUE}=== Benchmark 4: Concurrent Request Handling ===${NC}"
echo ""

echo -e "${YELLOW}Testing 5 concurrent requests through router:${NC}"
START=$(date +%s.%N)
for i in {1..5}; do
  curl -s -X POST "$BASE_URL/v1/chat/completions" \
    -H "Content-Type: application/json" \
    "${AUTH_HEADER[@]}" \
    -d "{
      \"messages\": [{\"role\": \"user\", \"content\": \"Request $i: say hello\"}],
      \"max_tokens\": 20,
      \"stream\": false
    }" > /tmp/concurrent_$i.json 2>/dev/null &
done
wait
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
SUCCESSFUL=0
for i in {1..5}; do
  if jq -e '.choices[0].message' /tmp/concurrent_$i.json > /dev/null 2>&1; then
    SUCCESSFUL=$((SUCCESSFUL + 1))
  fi
done
echo "  Completed: $SUCCESSFUL/5 requests"
echo "  Total time: ${DURATION}s"
echo "  Avg time per request: $(echo "scale=2; $DURATION / 5" | bc)s"
rm -f /tmp/concurrent_*.json
echo ""

# Benchmark 5: Streaming Performance
echo -e "${BLUE}=== Benchmark 5: Streaming Latency ===${NC}"
echo ""

echo -e "${YELLOW}Streaming through router (time to first token):${NC}"
START=$(date +%s.%N)
FIRST_CHUNK_TIME=""
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d '{
    "messages": [{"role": "user", "content": "Count to 10"}],
    "max_tokens": 50,
    "stream": true
  }' 2>/dev/null | while read line; do
    if [ -z "$FIRST_CHUNK_TIME" ]; then
      FIRST_CHUNK_TIME=$(date +%s.%N)
      TTFT=$(echo "$FIRST_CHUNK_TIME - $START" | bc)
      echo "  Time to First Token: ${TTFT}s"
    fi
  done
echo ""

# GPU Memory Usage
echo -e "${BLUE}=== GPU Memory Usage ===${NC}"
echo ""
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv,noheader,nounits | \
  awk '{printf "  Used: %d MB\n  Free: %d MB\n  Total: %d MB\n  Utilization: %.1f%%\n", $1, $2, $3, ($1/$3)*100}'
echo ""

# Benchmark 6: Routing Classification Overhead
# Compares explicit routing (no classification) vs auto-routing (with classification)
# to isolate the time spent on the Orchestrator 8B classification step.
echo -e "${BLUE}=== Benchmark 6: Routing Classification Overhead ===${NC}"
echo ""

echo -e "${YELLOW}Measuring classification overhead:${NC}"
# Explicit route — bypasses Orchestrator 8B classification
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/api/route" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d '{"route": "primary", "data": {"messages": [{"role": "user", "content": "Hello"}], "max_tokens": 5}}' > /dev/null
END=$(date +%s.%N)
DIRECT_TIME=$(echo "$END - $START" | bc)

# Auto-routed — includes Orchestrator 8B classification
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  "${AUTH_HEADER[@]}" \
  -d '{"messages": [{"role": "user", "content": "Hello"}], "max_tokens": 5}' > /dev/null
END=$(date +%s.%N)
ROUTED_TIME=$(echo "$END - $START" | bc)

OVERHEAD=$(echo "$ROUTED_TIME - $DIRECT_TIME" | bc)
echo "  Explicit route (no classification): ${DIRECT_TIME}s"
echo "  Auto-routed (with classification): ${ROUTED_TIME}s"
echo "  Classification overhead: ${OVERHEAD}s"
echo ""

# Model Information
echo -e "${BLUE}=== Routing Architecture ===${NC}"
echo ""

echo -e "${YELLOW}Models (via /v1/models):${NC}"
curl -s "${AUTH_HEADER[@]}" "$BASE_URL/v1/models" | jq -r '.data[].id' 2>/dev/null | while read id; do
  echo "  • $id"
done
echo ""

echo -e "${YELLOW}Routes:${NC}"
echo "  • MODERATE → Nano 30B (local, classified by Orchestrator 8B)"
echo "  • COMPLEX  → Grok (xAI API)"
echo "  • ENRICH   → Grok context fetch → Nano 30B"
echo "  • META     → Nano 30B (skips classification)"
echo ""

# Summary
echo -e "${BLUE}==================================="
echo "Benchmark Complete"
echo "===================================${NC}"
echo ""
echo -e "${GREEN}Four-route routing system is operational!${NC}"
echo ""
echo "Architecture:"
echo "  • MODERATE → Nano 30B: Local queries (classified by Orchestrator 8B)"
echo "  • COMPLEX  → Grok: Research-level complexity (xAI API)"
echo "  • ENRICH   → Grok context fetch → Nano 30B: Real-time information"
echo "  • META     → Nano 30B: Client meta-prompts (skips classification)"
echo ""
echo "Next steps:"
echo "  • Run 'make test' to verify routing"
echo "  • Monitor GPU: nvidia-smi"
echo "  • Adjust classification prompts in config/"
echo ""

# Cleanup
rm -f /tmp/moderate_short_response.json /tmp/moderate_response.json /tmp/complex_response.json /tmp/enrich_response.json
rm -f /tmp/primary_short_throughput.json /tmp/primary_long_throughput.json
rm -f /tmp/primary_context.json
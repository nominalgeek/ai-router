#!/bin/bash
# Benchmark script for RTX PRO 6000 Blackwell setup

set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

BASE_URL="http://localhost"

echo -e "${BLUE}==================================="
echo "AI Router Benchmark Suite"
echo "Five-Route Routing Pipeline"
echo "===================================${NC}"
echo ""

# Check GPU
echo -e "${YELLOW}GPU Status:${NC}"
nvidia-smi --query-gpu=name,memory.total,memory.free,temperature.gpu,utilization.gpu --format=csv,noheader
echo ""

# Warm up models
echo -e "${YELLOW}Warming up models (compiling CUDA kernels)...${NC}"
echo ""

# Warm up all three tiers
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "hi"}], "max_tokens": 5}' > /dev/null 2>&1

curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Explain AI"}], "max_tokens": 10}' > /dev/null 2>&1

echo -e "${GREEN}✓ Warmup complete${NC}"
echo ""

# Benchmark 1: Routing Latency by Category
echo -e "${BLUE}=== Benchmark 1: Routing Latency by Category ===${NC}"
echo ""

echo -e "${YELLOW}SIMPLE Query (Nano 30B via primary):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 10,
    "stream": false
  }' > /tmp/simple_response.json
END=$(date +%s.%N)
LATENCY=$(echo "$END - $START" | bc)
MODEL=$(cat /tmp/simple_response.json | jq -r '.model' 2>/dev/null || echo "Error")
RESPONSE=$(cat /tmp/simple_response.json | jq -r '.choices[0].message.content // .choices[0].message.reasoning_content' 2>/dev/null || echo "Error")
echo "  Latency: ${LATENCY}s"
echo "  Model: ${MODEL}"
echo "  Response: ${RESPONSE:0:100}"
echo ""

echo -e "${YELLOW}MODERATE Query (Nano 30B via router):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Explain how neural networks learn"}],
    "max_tokens": 50,
    "stream": false
  }' > /tmp/moderate_response.json
END=$(date +%s.%N)
LATENCY=$(echo "$END - $START" | bc)
MODEL=$(cat /tmp/moderate_response.json | jq -r '.model' 2>/dev/null || echo "Error")
RESPONSE=$(cat /tmp/moderate_response.json | jq -r '.choices[0].message.content // .choices[0].message.reasoning_content' 2>/dev/null || echo "Error")
echo "  Latency: ${LATENCY}s"
echo "  Model: ${MODEL}"
echo "  Response: ${RESPONSE:0:100}"
echo ""

echo -e "${YELLOW}COMPLEX Query (xAI Grok via router):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Design a novel approach to quantum error correction"}],
    "max_tokens": 100,
    "stream": false
  }' > /tmp/complex_response.json
END=$(date +%s.%N)
LATENCY=$(echo "$END - $START" | bc)
MODEL=$(cat /tmp/complex_response.json | jq -r '.model' 2>/dev/null || echo "Error")
RESPONSE=$(cat /tmp/complex_response.json | jq -r '.choices[0].message.content // .choices[0].message.reasoning_content' 2>/dev/null || echo "Error")
echo "  Latency: ${LATENCY}s"
echo "  Model: ${MODEL}"
echo "  Response: ${RESPONSE:0:100}"
echo ""

echo -e "${YELLOW}ENRICH Query (xAI context → Nano 30B):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "What are the latest developments in AI regulation this week?"}],
    "max_tokens": 200,
    "stream": false
  }' > /tmp/enrich_response.json
END=$(date +%s.%N)
LATENCY=$(echo "$END - $START" | bc)
MODEL=$(cat /tmp/enrich_response.json | jq -r '.model' 2>/dev/null || echo "Error")
RESPONSE=$(cat /tmp/enrich_response.json | jq -r '.choices[0].message.content // .choices[0].message.reasoning_content' 2>/dev/null || echo "Error")
echo "  Latency: ${LATENCY}s (includes xAI context fetch + primary inference)"
echo "  Model: ${MODEL}"
echo "  Response: ${RESPONSE:0:100}"
echo ""

# Benchmark 2: Throughput Test
echo -e "${BLUE}=== Benchmark 2: Throughput (Tokens/Second) ===${NC}"
echo ""

echo -e "${YELLOW}Router Model (100 tokens):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/router/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Write a short paragraph about artificial intelligence"}],
    "max_tokens": 100,
    "stream": false
  }' > /tmp/router_throughput.json
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
TOKENS=$(cat /tmp/router_throughput.json | jq -r '.usage.completion_tokens' 2>/dev/null || echo "0")
TPS=$(echo "scale=2; $TOKENS / $DURATION" | bc)
echo "  Duration: ${DURATION}s"
echo "  Tokens: ${TOKENS}"
echo "  Throughput: ${TPS} tokens/sec"
echo ""

echo -e "${YELLOW}Primary Model (300 tokens):${NC}"
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/primary/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Explain quantum computing in detail"}],
    "max_tokens": 300,
    "stream": false
  }' > /tmp/primary_throughput.json
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
TOKENS=$(cat /tmp/primary_throughput.json | jq -r '.usage.completion_tokens' 2>/dev/null || echo "0")
TPS=$(echo "scale=2; $TOKENS / $DURATION" | bc)
echo "  Duration: ${DURATION}s"
echo "  Tokens: ${TOKENS}"
echo "  Throughput: ${TPS} tokens/sec"
echo ""

# Benchmark 3: Context Length Test
echo -e "${BLUE}=== Benchmark 3: Long Context Performance ===${NC}"
echo ""

echo -e "${YELLOW}Primary Model (8K context):${NC}"
LONG_PROMPT=$(python3 -c "print('The story continues. ' * 500)")
START=$(date +%s.%N)
# No max_tokens — matches how the router sends requests to the primary model.
# The Nano 30B reasoning model uses its token budget for <think> blocks;
# any artificial cap risks truncating reasoning before content is emitted.
curl -s -X POST "$BASE_URL/primary/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d "{
    \"messages\": [{\"role\": \"user\", \"content\": \"${LONG_PROMPT:0:4000} Summarize the above.\"}],
    \"stream\": false
  }" > /tmp/primary_context.json 2>/dev/null
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
# Check both content and reasoning_content — the reasoning model may put
# all output in reasoning_content with content=null.
CONTENT=$(jq -r '.choices[0].message.content // empty' /tmp/primary_context.json 2>/dev/null)
REASONING=$(jq -r '.choices[0].message.reasoning_content // empty' /tmp/primary_context.json 2>/dev/null)
if [ ${#CONTENT} -gt 10 ] || [ ${#REASONING} -gt 10 ]; then
  echo -e "  ${GREEN}✓ Success${NC} (${DURATION}s)"
else
  echo -e "  ${RED}✗ Failed${NC}"
fi
echo ""

# Benchmark 4: Concurrent Requests
echo -e "${BLUE}=== Benchmark 4: Concurrent Request Handling ===${NC}"
echo ""

echo -e "${YELLOW}Testing 10 concurrent requests to router model:${NC}"
START=$(date +%s.%N)
for i in {1..10}; do
  curl -s -X POST "$BASE_URL/router/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -d "{
      \"messages\": [{\"role\": \"user\", \"content\": \"Request $i\"}],
      \"max_tokens\": 20,
      \"stream\": false
    }" > /tmp/concurrent_$i.json 2>/dev/null &
done
wait
END=$(date +%s.%N)
DURATION=$(echo "$END - $START" | bc)
SUCCESSFUL=$(ls /tmp/concurrent_*.json 2>/dev/null | wc -l)
echo "  Completed: $SUCCESSFUL/10 requests"
echo "  Total time: ${DURATION}s"
echo "  Avg time per request: $(echo "scale=2; $DURATION / 10" | bc)s"
rm -f /tmp/concurrent_*.json
echo ""

# Benchmark 5: Streaming Performance
echo -e "${BLUE}=== Benchmark 5: Streaming Latency ===${NC}"
echo ""

echo -e "${YELLOW}Router Model (streaming):${NC}"
START=$(date +%s.%N)
FIRST_CHUNK_TIME=""
curl -s -X POST "$BASE_URL/router/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Count to 10"}],
    "max_tokens": 50,
    "stream": true
  }' 2>/dev/null | while read line; do
    if [ -z "$FIRST_CHUNK_TIME" ]; then
      FIRST_CHUNK_TIME=$(date +%s.%N)
      TTFT=$(echo "$FIRST_CHUNK_TIME - $START" | bc)
      echo "  Time to First Token: ${TTFT}s"
    fi
  done
echo ""

# GPU Memory Usage
echo -e "${BLUE}=== GPU Memory Usage ===${NC}"
echo ""
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv,noheader,nounits | \
  awk '{printf "  Used: %d MB\n  Free: %d MB\n  Total: %d MB\n  Utilization: %.1f%%\n", $1, $2, $3, ($1/$3)*100}'
echo ""

# Benchmark 6: Routing Decision Overhead
echo -e "${BLUE}=== Benchmark 6: Prompt-Based Routing Overhead ===${NC}"
echo ""

echo -e "${YELLOW}Measuring routing classification time:${NC}"
# Direct call to Orchestrator 8B (no routing)
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/router/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Hello"}], "max_tokens": 5}' > /dev/null
END=$(date +%s.%N)
DIRECT_TIME=$(echo "$END - $START" | bc)

# Call through router (with routing classification)
START=$(date +%s.%N)
curl -s -X POST "$BASE_URL/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Hello"}], "max_tokens": 5}' > /dev/null
END=$(date +%s.%N)
ROUTED_TIME=$(echo "$END - $START" | bc)

OVERHEAD=$(echo "$ROUTED_TIME - $DIRECT_TIME" | bc)
echo "  Direct call: ${DIRECT_TIME}s"
echo "  Routed call: ${ROUTED_TIME}s"
echo "  Routing overhead: ${OVERHEAD}s"
echo ""

# Model Information
echo -e "${BLUE}=== Routing Architecture ===${NC}"
echo ""

echo -e "${YELLOW}SIMPLE - Router (Orchestrator 8B):${NC}"
curl -s "$BASE_URL/router/v1/models" | jq -r '.data[0] | "  Model: \(.id)\n  Role: Classification only"' 2>/dev/null || echo "  Error fetching model info"
echo ""

echo -e "${YELLOW}MODERATE - Primary (Nano 30B):${NC}"
curl -s "$BASE_URL/primary/v1/models" | jq -r '.data[0] | "  Model: \(.id)\n  Role: Moderate complexity queries"' 2>/dev/null || echo "  Error fetching model info"
echo ""

echo -e "${YELLOW}COMPLEX - xAI (Grok):${NC}"
echo "  Model: grok-4-1-fast-reasoning"
echo "  Role: Highly complex research queries"
echo ""

echo -e "${YELLOW}ENRICH - xAI context → Primary (Nano 30B):${NC}"
echo "  Context: grok-4-1-fast-reasoning (retrieval)"
echo "  Inference: Nano 30B (with injected context)"
echo "  Role: Queries requiring current/real-time information"
echo ""

echo -e "${YELLOW}META - Primary (Nano 30B):${NC}"
echo "  Model: Nano 30B"
echo "  Role: Client-generated meta-prompts (follow-ups, titles, summaries)"
echo "  Note: Skips classification — detected by message structure heuristics"
echo ""

# Summary
echo -e "${BLUE}==================================="
echo "Benchmark Complete"
echo "===================================${NC}"
echo ""
echo -e "${GREEN}Five-route routing system is operational!${NC}"
echo ""
echo "Architecture:"
echo "  • SIMPLE   → Nano 30B: Simple queries (classified by Orchestrator 8B)"
echo "  • MODERATE → Nano 30B: Moderate complexity (local)"
echo "  • COMPLEX  → Grok: Research-level complexity (xAI API)"
echo "  • ENRICH   → Grok context fetch → Nano 30B: Real-time information"
echo "  • META     → Nano 30B: Client meta-prompts (skips classification)"
echo ""
echo "Next steps:"
echo "  • Run 'make test' to verify routing"
echo "  • Monitor GPU: nvidia-smi"
echo "  • Adjust classification prompts in config/"
echo ""

# Cleanup
rm -f /tmp/simple_response.json /tmp/moderate_response.json /tmp/complex_response.json /tmp/enrich_response.json
rm -f /tmp/router_throughput.json /tmp/primary_throughput.json
rm -f /tmp/primary_context.json
# Session Review Report
**Date**: 2026-02-16
**Sessions reviewed**: 7
**Period**: 2026-02-16T00:41:23-08:00 to 2026-02-16T00:42:24-08:00

## Summary
- Total sessions: 7
- By route: primary=4, xai=1, enrich=1, meta=1
- Errors: 0
- Classification errors: 0
- Issues found: 3

All 7 sessions are single-turn requests spanning ~1 minute, likely from the test suite. This batch exercises all four route types (SIMPLE, MODERATE, COMPLEX, ENRICH) plus meta detection. All classifications succeeded — no HTTP 400 context overflow errors (though these are single-turn so the previous bug would not have triggered).

## Issues

### Null Content / Reasoning-Only Responses: Model spends entire token budget on chain-of-thought
**Severity**: high
**Sessions affected**: c12c5dac, 22fd8fbc
**Details**: Two sessions returned `content: null` because the primary model (Nemotron Nano 30B) spent all available tokens on its internal chain-of-thought (`reasoning_content`) and never produced visible output.

- **c12c5dac** (meta: follow-up suggestions): `max_tokens: 100`, `finish_reason: "length"`. The model's reasoning consumed all 100 tokens deliberating about what follow-up questions to generate, never producing the actual JSON output.

- **22fd8fbc** (MODERATE: Python code review): `max_tokens: 300`, `finish_reason: "length"`. Same pattern — the model spent 300 tokens on reasoning and returned `content: null`.

Both sessions had client-supplied `max_tokens` values that were too low for a reasoning model. The enrich route already has an `ENRICH_MIN_MAX_TOKENS` floor (visible in `app.py:117-120`) to prevent exactly this issue, but no equivalent floor exists for primary or meta routes.

**Recommendation**: Extend the `max_tokens` floor logic to all routes, not just enrich. A floor of ~1024 tokens for primary routes and ~512 for meta routes would prevent reasoning-only responses while still respecting higher client values. This could be a per-route config in `config.py` (e.g., `PRIMARY_MIN_MAX_TOKENS`, `META_MIN_MAX_TOKENS`).

---

### Truncated Responses: Low client max_tokens cutting off generation mid-sentence
**Severity**: medium
**Sessions affected**: 2831ae96, 987261d4, d58706c4, e766f69a
**Details**: Four sessions have responses truncated mid-sentence due to low client-supplied `max_tokens`:

| Session | Route | max_tokens | Truncated at |
|---------|-------|-----------|-------------|
| 2831ae96 | primary (SIMPLE) | 50 | "Greetings! This is **NAME_1**, the" |
| 987261d4 | primary (MODERATE) | 200 | "If a" |
| d58706c4 | xai (COMPLEX) | 100 | "a" |
| e766f69a | enrich | 500 | "1" (after weather data) |

This is the same underlying issue: client-supplied parameters are passed through without bounds. The `ENRICH_MIN_MAX_TOKENS` floor helped session e766f69a get 500 tokens, but no floor exists for other routes.

**Recommendation**: Implement per-route minimum `max_tokens` floors. Suggested minimums: SIMPLE=256, MODERATE=1024, COMPLEX=2048, META=512, ENRICH=1024 (current).

---

### Model Identity Leak: Primary model outputs placeholder name
**Severity**: low
**Sessions affected**: 2831ae96
**Details**: When responding to "Hello", the primary model replied: *"Greetings! This is **NAME_1**, the"* — `NAME_1` is a training data artifact where personal names were replaced with placeholder tokens during fine-tuning. This would appear to the user as a broken response.

**Recommendation**: The primary system prompt (`config/prompts/primary/system.md`) could include guidance like *"Never use placeholder names like NAME_1."* — though this adds prompt overhead for an edge case. Increasing `max_tokens` (per the recommendation above) would at least ensure the full response is visible for diagnosis.

## Route Quality Summary

### PRIMARY — SIMPLE (1 session: 2831ae96)
- Classification: correct. "Hello" → SIMPLE in 1,157ms.
- Response: truncated at 50 tokens. Contains NAME_1 artifact.
- Total time: 2,091ms.

### PRIMARY — MODERATE (2 sessions: 987261d4, e26f04a1)
- Classification: correct for both. Average classification time: 2,244ms.
- e26f04a1: Good response quality (markdown table comparing list vs dict) at 300 tokens.
- 987261d4: Good start but truncated at 200 tokens.
- Average total time: 6,849ms.

### XAI — COMPLEX (1 session: d58706c4)
- Classification: correct. "Design a novel quantum-resistant cryptographic algorithm" → COMPLEX in 1,283ms.
- Response: creative answer but truncated at 100 tokens.
- Total time: 11,344ms.

### ENRICH (1 session: e766f69a)
- Classification: correct. "Current weather in Tokyo right now" → ENRICH in 1,205ms.
- Enrichment: xAI retrieved accurate, current weather data with source citations. 21,028ms.
- Primary response: successfully incorporated enrichment context. Truncated at 500 tokens but delivered key information.
- Total time: 31,374ms.

### META (1 session: c12c5dac)
- Detection: correct. 0ms classification.
- Response: **Failed** — content=null due to 100 max_tokens consumed entirely by reasoning.
- Total time: 1,839ms.

## Prompt Improvement Suggestions

No prompt changes warranted. All classifications were accurate across all categories, and meta detection worked flawlessly.

The issues in this batch are all infrastructure-level:
1. **Per-route `max_tokens` floors** — extend the existing `ENRICH_MIN_MAX_TOKENS` pattern to all routes
2. **Response logging** — sessions c12c5dac and 22fd8fbc stored raw JSON response bodies in `response_content` instead of extracted text, suggesting the content extraction at `providers.py:362-371` may not be handling `content: null` cases correctly in all code paths

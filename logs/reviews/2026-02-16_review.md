# Session Review Report
**Date**: 2026-02-16
**Sessions reviewed**: 14
**Period**: 2026-02-16T00:19:37-08:00 to 2026-02-16T00:28:05-08:00

## Summary
- Total sessions: 14
- By route: primary=4, xai=0, enrich=2, meta=8
- Errors: 4 (all classification HTTP 400s)
- Issues found: 3

All 14 sessions originate from a single Open WebUI conversation about the video game Escape from Tarkov. The conversation spanned ~9 minutes. User queries alternated with Open WebUI's automated meta-prompts (follow-up suggestions, title generation, tag generation).

## Issues

### Classification Context Overflow: Router model returns HTTP 400 on longer conversations
**Severity**: high
**Sessions affected**: 96b30e18, 3e95aeb0, c49a88d8, db1616e7
**Details**: Four consecutive classification requests returned HTTP 400 with only 7ms latency, indicating the router model (Nemotron Orchestrator 8B, 2,048-token context window) rejected the request immediately — almost certainly because the input exceeded its context length.

The code in `providers.py:82-118` builds a conversation context prefix with a budget of `CLASSIFY_CONTEXT_BUDGET` = 4,000 characters (~1,000 tokens). This is combined with:
- The classification system prompt (~600 chars / ~150 tokens)
- The routing request template (~1,600 chars / ~400 tokens)
- `max_tokens: 512` reserved for output

Estimated total: ~1,550 input tokens + 512 output tokens = ~2,062 tokens — right at the 2,048-token limit. The first two classifications worked because the conversation was short (0 and 2 prior messages). By the third user turn, prior messages included long assistant responses about Escape from Tarkov (game overview, roadmap details), which pushed the context over the limit.

The fallback behavior (defaulting to `primary` route) is reasonable, but it means these queries lost the chance to be routed to `enrich` or `xai` — and at least two of them (96b30e18: "Are there any confirmed timelines for the upcoming 'Scav Life' DLC storyline?" and c49a88d8: "When is a beta or test server expected for the Scav Life DLC?") clearly needed real-time information.

**Recommendation**: Reduce `CLASSIFY_CONTEXT_BUDGET` from 4,000 to 2,000 characters (~500 tokens). This gives a comfortable margin:
- System prompt: ~150 tokens
- Context prefix: ~500 tokens
- Routing template + query: ~400 tokens
- Output (max_tokens): 512
- **Total: ~1,562 tokens** (well within 2,048)

The classifier only needs enough context to resolve references ("that game", "the DLC") — it doesn't need full assistant responses. Even 2,000 chars of the most recent turns should provide sufficient reference resolution. Alternatively, consider truncating only assistant messages in the context (e.g., first 200 chars each) while keeping user messages intact, since user messages contain the actual topics being discussed.

---

### Missed ENRICH on Fallback: Queries needing real-time data routed to primary without enrichment
**Severity**: medium
**Sessions affected**: 96b30e18, c49a88d8
**Details**: Due to the classification 400 errors above, these two queries fell back to the primary model without enrichment:
- 96b30e18: "Are there any confirmed timelines for the upcoming 'Scav Life' DLC storyline?" — asks about confirmed timelines, clearly needs current information
- c49a88d8: "When is a beta or test server expected for the Scav Life DLC?" — asks about expected dates, needs current information

The primary model (Nano 30B) likely answered from training data or hallucinated timelines, since it had no injected real-time context. This is the worst-case failure mode: the user gets a confident-sounding answer that may be factually wrong, with no indication that enrichment was skipped.

**Recommendation**: This is a downstream effect of the context overflow issue above. Fixing the context budget resolves this. Additionally, consider logging a user-visible warning when classification fails and the query falls back — or adding a heuristic keyword check (e.g., "when", "timeline", "expected", "upcoming" + proper nouns → force ENRICH) as a secondary fallback before defaulting to primary.

---

### Borderline ENRICH: General game knowledge query triggered full enrichment pipeline
**Severity**: low
**Sessions affected**: 89483395
**Details**: The query "what can you tell me about the game escape from tarkov?" was classified as ENRICH, triggering a full enrichment round-trip through xAI (20,110ms) before reaching the primary model. The routing prompt's ENRICH definition includes "factual details about specific real-world entities" and examples like "What is [company name] known for?" — so this classification is technically correct per the current prompt.

The enrichment did add genuine value: current patch info (1.0.2.0, Feb 10 2026), the Nov 2025 full release date, and recent community activity. Total request time was 22,275ms (vs. ~8,000ms for a direct primary route).

**Recommendation**: No change needed. The classification followed the prompt correctly, and the enrichment improved answer quality. This is the intended behavior for entity-specific queries. If xAI API costs become a concern, the ENRICH definition could be narrowed to require *explicit* temporal markers, but that would miss legitimate cases like this one.

## Route Quality Summary

### ENRICH (2 sessions: 89483395, d977393f)
- Both correctly classified — queries about a specific game and its future roadmap
- Average classification time: 1,949ms (within normal range)
- Average enrichment time: 22,596ms (dominated by xAI API call with web+X search)
- Average total time: 24,559ms
- xAI enrichment content was high quality and current in both cases
- No issues with context injection or primary model response

### PRIMARY (4 sessions: 96b30e18, 3e95aeb0, c49a88d8, db1616e7)
- All 4 arrived via classification error fallback, not intentional routing
- 0 sessions were intentionally classified as SIMPLE or MODERATE
- At least 2 (96b30e18, c49a88d8) should have been ENRICH
- 1 (db1616e7: "summarize our chat") is correctly SIMPLE/MODERATE — no enrichment needed
- 1 (3e95aeb0: "Will the Scav Life DLC add new gameplay mechanics?") is borderline — could be ENRICH (needs current info) or MODERATE (speculative regardless)

### META (8 sessions: 06be46bf, 205a63ab, 73ebb81f, b5a7030e, 96d788ab, 218039e8, 965fac3e, abb9515f)
- All correctly detected via heuristic (classification_ms: 0) — no false positives or misses
- Types: 6 follow-up suggestion tasks, 1 title generation, 1 tag generation
- Average response time: 10,761ms
- All returned valid JSON in the expected format
- Meta detection is working reliably

### XAI (0 sessions)
- No queries were classified as COMPLEX in this batch
- Not enough data to evaluate COMPLEX routing quality

## Prompt Improvement Suggestions

No prompt changes are warranted from this batch. The classification prompts are working correctly — the issues stem from the router model's context window being exceeded, not from classification logic.

The one actionable change is infrastructure-level: reduce `CLASSIFY_CONTEXT_BUDGET` in `config.py` (or via env var) from 4,000 to 2,000 to prevent context overflow on multi-turn conversations. This is a config change, not a prompt change.

If future sessions show the classifier struggling to resolve references with a smaller context budget, consider a hybrid approach: truncate assistant messages to ~200 chars while preserving full user messages within the budget.

# Session Review Report
**Date**: 2026-02-16
**Sessions reviewed**: 7
**Period**: 2026-02-16T10:55:33-08:00 to 2026-02-16T10:57:06-08:00

## Summary
- Total sessions: 7
- By route: primary=4, xai=1, enrich=1, meta=1
- Errors: 0 (no HTTP errors)
- Issues found: 3

This is the fourth review of the same day. The per-route `max_tokens` floors recommended in reviews 02 and 03 have been implemented (`PRIMARY_MIN_MAX_TOKENS=2048`, `ENRICH_MIN_MAX_TOKENS=4096`). The app.log confirms these floors are being applied — every primary-route request is raised to at least 2048, and the enrich route to 4096. Despite this, **all 6 sessions hitting the primary model still return `content: null`** because the Nano 30B reasoning model exhausts even these larger budgets on chain-of-thought. The token floors are necessary but not sufficient; the root cause is that the model has no mechanism to reserve tokens for visible output.

## Issues

### 1. Null Content / Reasoning-Only Responses: Primary model returns content=null on all 6 primary-model sessions
**Severity**: critical
**Sessions affected**: c750d1b5, bf9ebe5f, b0aa4651, af8ad3c1, a6fcd92c, a5532d8a
**Details**: Every session routed to the primary model returned `content: null` with `finish_reason: "length"`. The model consumed its entire `max_tokens` budget on reasoning and never produced visible output. This is a 100% failure rate for user-facing content in this batch.

| Session | Route | Query (truncated) | max_tokens | completion_tokens | content |
|---------|-------|--------------------|-----------|-------------------|---------|
| c750d1b5 | primary (SIMPLE) | "Hello" | 2048 | 2048 | null |
| bf9ebe5f | primary (fallback) | "Explain quantum entanglement in detail" | 2048 | 2048 | null |
| b0aa4651 | enrich | "What is the current weather in Tokyo right now?" | 4096 | 4096 | null |
| af8ad3c1 | meta | Follow-up question generation | 2048 | 2048 | null |
| a6fcd92c | primary (MODERATE) | Python code review | 2048 | 2048 | null |
| a5532d8a | primary (MODERATE) | Python dict vs list explanation | 2048 | 2048 | null |

**Progress since review_03**: The token floors are now in effect — the app.log shows "raising max_tokens from 50 to 2048", "raising max_tokens from 500 to 4096", etc. This is a clear improvement over the previous batch where some sessions had as few as 50 max_tokens. But the model still fills the entire budget with reasoning.

**Recommendation**: The token floor approach has been exhausted as a standalone fix. The model needs either:
1. **A much higher floor** — try `PRIMARY_MIN_MAX_TOKENS=8192` and `ENRICH_MIN_MAX_TOKENS=16384`. The Nano 30B has a 32K context window, so there's room. The reasoning overhead appears to scale with prompt complexity (a simple "Hello" consumed 2048 tokens of reasoning, but more complex queries may need proportionally more). Setting the floor high enough that reasoning finishes with room to spare is the brute-force approach.
2. **A prompt-level instruction to limit reasoning** — Add to the primary system prompt something like: "Keep your internal reasoning concise. Prioritize producing a visible answer over exhaustive deliberation." This would need testing since it edits `config/prompts/primary/system.md` which is outside the scope of safe fixes per AGENT.md.
3. **Investigate vLLM reasoning budget controls** — Some vLLM configurations support `--max-reasoning-tokens` or similar parameters that cap the `<think>` block and reserve tokens for content. If available for this model, this would be the cleanest solution.

This is now the most critical operational issue — the router is correctly classifying and routing, the enrichment pipeline works perfectly, but the final generation step produces empty responses 100% of the time.

---

### 2. Classification Truncation: Classifier reasoning truncated before producing a label
**Severity**: medium
**Sessions affected**: bf9ebe5f
**Details**: Session bf9ebe5f ("Explain the concept of quantum entanglement in detail") has `classification_raw: ""` (empty string). The classifier's response was truncated mid-reasoning at 512 tokens:

> "...which is more aligne"

The app.log confirms: `Routing classification unclear: '', defaulting to primary`. The fallback to primary was a reasonable outcome (this is a MODERATE query), but the classification was unintentional.

This is the same pattern as session 5c46727d in review_03 — the same query ("Explain quantum entanglement") causing the classifier to deliberate at length between MODERATE and COMPLEX, exhausting the 512-token budget. This is now a recurring pattern for this specific query type.

**Recommendation**: Since this has now recurred across two separate batches with the same query, consider increasing `CLASSIFY_MAX_TOKENS` from 512 to 768. Alternatively, add a brevity instruction to the routing system prompt (e.g., "Classify quickly — do not deliberate at length"). This would be a safe, additive prompt change per AGENT.md Step 4 rules, but only one instance exists in this batch (below the 3-session threshold for self-service fixes).

---

### 3. Truncated xAI Response: COMPLEX query response cut off at 100 tokens
**Severity**: medium
**Sessions affected**: a7a2a5b1
**Details**: Session a7a2a5b1 ("Design a novel quantum-resistant cryptographic algorithm") was correctly classified as COMPLEX and routed to xAI. However, the request was sent with `max_tokens: 100`, which is the client-supplied value passed through without a floor. The response was truncated mid-sentence:

> "...Extends Module-L"

The Grok model began a creative, substantive response ("QuatSecure: A Novel Quaternion-Based Post-Quantum KEM") but was cut off after barely starting.

This is the third time this exact issue has appeared across reviews (review_02 session d58706c4, review_03 session 90b717a7, now a7a2a5b1) — all with the same COMPLEX query and 100 max_tokens.

**Recommendation**: Implement a `XAI_MIN_MAX_TOKENS` floor (suggested value: 4096) for requests forwarded to xAI, mirroring the existing `PRIMARY_MIN_MAX_TOKENS` and `ENRICH_MIN_MAX_TOKENS` pattern. This is now a 3-occurrence pattern across reviews and should be prioritized. The code change would be in `providers.py` in the `forward_request()` function, following the same pattern as the existing primary and enrich floors.

## Route Quality Summary

### PRIMARY — SIMPLE (1 session: c750d1b5)
- **Classification**: Correct. "Hello" → SIMPLE in 1,291ms.
- **Response**: content=null at 2,048 max_tokens. Model spent 8,297ms generating nothing visible.
- **Total**: 9,588ms.

### PRIMARY — Fallback (1 session: bf9ebe5f)
- **Classification**: Failed — reasoning truncated at 512 tokens, empty label, defaulted to primary.
- **Response**: content=null at 2,048 max_tokens.
- **Total**: 10,895ms.

### PRIMARY — MODERATE (2 sessions: a6fcd92c, a5532d8a)
- **Classification**: Correct for both. Code review → MODERATE (2,370ms). Dict vs list → MODERATE (1,451ms).
- **Response**: Both returned content=null at 2,048 max_tokens.
- **Average total**: 10,364ms.
- **Note**: Session a5532d8a ("what is a dictionary vs a list") could arguably be SIMPLE, but MODERATE is defensible since the user asked for comparison and explanation. No action needed.

### XAI — COMPLEX (1 session: a7a2a5b1)
- **Classification**: Correct. "Design a novel quantum-resistant cryptographic algorithm" → COMPLEX in 1,510ms.
- **Response**: Began a strong answer but truncated at 100 tokens (client passthrough, no floor).
- **Total**: 12,668ms. xAI call took 11,158ms (within normal range for reasoning model).
- **This is the only session that produced any visible content**, and it was truncated.

### ENRICH (1 session: b0aa4651)
- **Classification**: Correct. "Current weather in Tokyo right now" → ENRICH in 589ms (fastest classification).
- **Enrichment**: Excellent. xAI retrieved accurate, sourced weather data (temperature, wind, humidity, forecast with citations) in 23,604ms. Context was 1,498 chars.
- **Response**: content=null at 4,096 max_tokens despite having good enrichment context injected. The enrichment pipeline works perfectly — the final generation fails.
- **Total**: 40,931ms.

### META (1 session: af8ad3c1)
- **Detection**: Correct. Follow-up generation task detected in 0ms (heuristic match on `### Task:` and `<chat_history>` markers). No classification needed.
- **Response**: content=null at 2,048 max_tokens.
- **Total**: 8,385ms.

## Latency Summary

| Metric | Value | Threshold | Status |
|--------|-------|-----------|--------|
| Avg classification_ms | 1,629ms (across 6 classified sessions) | <5,000ms | OK |
| Max classification_ms | 2,562ms (bf9ebe5f) | <5,000ms | OK |
| Enrichment step | 23,604ms (b0aa4651) | <60,000ms | OK |
| Slowest total_ms | 40,931ms (enrich route) | <90,000ms | OK |

No latency outliers in this batch. All timing is within acceptable bounds.

## Prompt Improvement Suggestions

No prompt changes applied. The classification prompts are working well — all 6 classifications that completed were accurate. The one failure was a token-budget issue, not a prompt issue.

**Infrastructure changes recommended (priority order)**:

1. **Increase primary model token floor significantly** — `PRIMARY_MIN_MAX_TOKENS=8192` or higher. The current 2048 is consumed entirely by reasoning. This is the single highest-impact change.
2. **Add xAI token floor** — `XAI_MIN_MAX_TOKENS=4096`. This has been a recurring issue across 3 reviews and is a simple code change following the existing pattern.
3. **Increase enrich floor** — `ENRICH_MIN_MAX_TOKENS=16384`. Even 4096 was insufficient with enrichment context inflating the prompt.
4. **Investigate vLLM reasoning budget controls** — If the model/runtime supports capping reasoning tokens, this would address the root cause across all routes.
5. **Consider increasing `CLASSIFY_MAX_TOKENS` to 768** — The quantum entanglement classification truncation has now recurred twice.

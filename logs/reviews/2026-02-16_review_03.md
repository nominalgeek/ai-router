# Session Review Report
**Date**: 2026-02-16
**Sessions reviewed**: 7
**Period**: 2026-02-16T10:11:12-08:00 to 2026-02-16T10:11:55-08:00

## Summary
- Total sessions: 7
- By route: primary=4, xai=1, enrich=1, meta=1
- Errors: 0 (no HTTP errors)
- Issues found: 3

All 7 sessions are single-turn requests spanning under 1 minute, likely from the integration test suite. All classifications returned HTTP 200. One classification produced an empty label due to reasoning truncation. Six of seven sessions returned `content: null` to the user because the primary model spent its entire token budget on chain-of-thought reasoning.

## Issues

### Null Content / Reasoning-Only Responses (Recurring): Primary model returns content=null on 6 of 7 sessions
**Severity**: high
**Sessions affected**: 3f6020d3, 5c46727d, f8522618, 587a54b7, de0570a7, 1f2c400b
**Details**: Six sessions returned `content: null` with `finish_reason: "length"` because the Nemotron Nano 30B model consumed the entire `max_tokens` budget on its internal chain-of-thought (`reasoning_content`) and never produced visible output. This means the user received empty responses for all of these queries.

| Session | Route | Query | max_tokens | completion_tokens | content |
|---------|-------|-------|-----------|-------------------|---------|
| 3f6020d3 | primary (SIMPLE) | "Hello" | 50 | 50 | null |
| 5c46727d | primary (fallback) | "Explain quantum entanglement in detail" | 200 | 200 | null |
| f8522618 | enrich | "What is the current weather in Tokyo right now?" | 1024 | 1024 | null |
| 587a54b7 | meta | Follow-up question generation | 100 | 100 | null |
| de0570a7 | primary (MODERATE) | Python code review | 300 | 300 | null |
| 1f2c400b | primary (MODERATE) | Python dict vs list explanation | 300 | 300 | null |

This is the same issue documented in review_02 (sessions c12c5dac, 22fd8fbc) but now affecting nearly every session. The previous review recommended per-route `max_tokens` floors. That recommendation has not yet been implemented, and this batch demonstrates the severity: **86% of requests returned empty responses**.

Notably, session f8522618 (enrich route) had `max_tokens: 1024` — suggesting the existing `ENRICH_MIN_MAX_TOKENS` floor is being applied — but even 1024 tokens was insufficient. The enrichment context injected into the system prompt was substantial (~700 prompt tokens from the weather data), and the model still spent all 1024 completion tokens on reasoning. This suggests the floor for enrich routes needs to be higher than the current value, or the model needs explicit instruction to limit reasoning length.

**Recommendation**: This is now a critical usability issue. Two actions needed:
1. **Implement per-route `max_tokens` floors** as recommended in review_02. Suggested minimums based on this batch: SIMPLE=512, MODERATE=2048, COMPLEX=4096, META=512, ENRICH=2048. The MODERATE floor needs to be higher than previously suggested (1024) because 300 tokens was completely consumed by reasoning for a straightforward code review.
2. **Increase the enrich floor specifically** — 1024 is not enough when enrichment context inflates the prompt. A floor of 2048 would give the model room for both reasoning and visible output.

---

### Classification Truncation: Classifier reasoning cut off before producing a label
**Severity**: medium
**Sessions affected**: 5c46727d
**Details**: Session 5c46727d ("Explain the concept of quantum entanglement in detail") has `classification_raw: ""` (empty string). The classifier's `response_content` shows extended reasoning that was truncated mid-sentence:

> "...might involve more than a basic explan"

The reasoning hit the 512 `max_tokens` limit before the model produced its final classification word. The `<think>` block ran for the full 512 tokens debating between MODERATE and COMPLEX but never emitted a label. The system defaulted to `primary` route, which was likely correct (this is a MODERATE query), but the classification was not intentional — it was a fallback.

For comparison, session 90b717a7 (a similarly complex classification decision) completed in 1,324ms with only ~250 tokens of reasoning before emitting "COMPLEX". Session 5c46727d took 2,562ms and exhausted all 512 tokens. The difference appears to be that the quantum entanglement query sits right on the MODERATE/COMPLEX boundary, causing the classifier to deliberate extensively.

**Recommendation**: This is an edge case but worth monitoring. The 512 `max_tokens` for classification is generally adequate (6 of 7 sessions classified successfully). If this pattern recurs across future batches, consider either:
- Increasing `CLASSIFY_MAX_TOKENS` from 512 to 768 to give more reasoning headroom
- Adding a note in the routing system prompt: "Make your classification quickly — do not deliberate at length"

---

### Truncated xAI Response: COMPLEX query answer cut off mid-sentence
**Severity**: low
**Sessions affected**: 90b717a7
**Details**: The only session that produced visible content was 90b717a7 (COMPLEX route via xAI). The Grok model began a detailed response about a novel "Quasar" KEM algorithm but was truncated at 100 tokens:

> "...enhance side-channel resistance and reduce key sizes compared to CR"

The `max_tokens: 100` is far too low for a COMPLEX query that asks for algorithm design. This is the same client-parameter-passthrough issue noted in review_02 (session d58706c4 had the identical problem with the identical query).

**Recommendation**: Same as issue #1 — per-route `max_tokens` floors. COMPLEX queries routed to xAI should have a floor of at least 4096 tokens to allow for substantive technical responses.

## Route Quality Summary

### PRIMARY — SIMPLE (1 session: 3f6020d3)
- Classification: correct. "Hello" → SIMPLE in 1,082ms. Fast, accurate.
- Response: **Failed** — content=null at 50 max_tokens.
- Total time: 14,818ms (high for a simple greeting — 13,736ms spent on generation that produced nothing).

### PRIMARY — MODERATE (2 sessions: de0570a7, 1f2c400b)
- Classification: correct for both. Average classification time: 1,532ms.
- de0570a7: Code review request, MODERATE correct. Response: content=null at 300 tokens.
- 1f2c400b: Dict vs list explanation, MODERATE correct (could arguably be SIMPLE but MODERATE is reasonable for "explain the key differences"). Response: content=null at 300 tokens.
- Average total time: 2,785ms.

### PRIMARY — Fallback (1 session: 5c46727d)
- Classification: **failed** — classifier reasoning truncated, empty label, defaulted to primary.
- The query ("Explain quantum entanglement in detail") would likely be MODERATE or COMPLEX. Default to primary is reasonable for MODERATE; if it should have been COMPLEX, it was under-escalated.
- Response: content=null at 200 max_tokens.
- Total time: 3,390ms.

### XAI — COMPLEX (1 session: 90b717a7)
- Classification: correct. "Design a novel quantum-resistant cryptographic algorithm" → COMPLEX in 1,324ms.
- Response: began a creative, well-structured answer but truncated at 100 tokens.
- Total time: 7,738ms. xAI provider call took 6,413ms (within normal range for reasoning model).

### ENRICH (1 session: f8522618)
- Classification: correct. "Current weather in Tokyo right now" → ENRICH in 548ms (fastest classification in the batch).
- Enrichment: xAI retrieved accurate, sourced weather data in 9,496ms. Context quality was excellent — temperature, conditions, wind, forecast with citations.
- Primary response: **Failed** — despite having good enrichment context injected, the model returned content=null at 1024 max_tokens. The enrichment pipeline worked perfectly but the final response was empty.
- Total time: 14,377ms.

### META (1 session: 587a54b7)
- Detection: correct. Follow-up generation task detected in 0ms (heuristic match on `### Task:` and `<chat_history>` markers).
- Response: content=null at 100 max_tokens. The model spent all tokens reasoning about what follow-up questions to generate.
- Total time: 418ms.

## Prompt Improvement Suggestions

No prompt changes are warranted from this batch. All classifications that completed were accurate:
- SIMPLE, MODERATE, COMPLEX, ENRICH, and META were each correctly identified when the classifier produced a label.
- The routing prompt examples and category definitions are functioning well.
- The one classification failure (5c46727d) was a token-budget issue, not a prompt-quality issue.

The critical issues in this batch are all infrastructure-level and match the recommendations from review_02:

1. **Per-route `max_tokens` floors** — This is now the single most important fix. 86% of sessions returned empty content. The existing `ENRICH_MIN_MAX_TOKENS` pattern should be extended to all routes with higher minimums than previously suggested, since even 1024 tokens proved insufficient for the enrich route.

2. **Consider a reasoning budget hint** — If the primary model supports any mechanism to limit reasoning token allocation (e.g., reserving a portion of max_tokens for visible content), this would address the root cause. Without this, the only lever is making max_tokens large enough that reasoning finishes before the budget runs out.

3. **Classification max_tokens** — Monitor for future truncation events like 5c46727d. If the pattern recurs, increase `CLASSIFY_MAX_TOKENS` from 512 to 768.
